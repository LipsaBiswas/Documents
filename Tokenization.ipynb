{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPchAvgpX9vqH46HDVG22ij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LipsaBiswas/Documents/blob/master/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaJ_CPSs0aN-"
      },
      "source": [
        "#https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/#:~:text=Tokenization%20is%20a%20common%20task%20in%20Natural%20Language%20Processing%20(NLP).&text=Tokens%20are%20the%20building%20blocks,words%2C%20characters%2C%20or%20subwords.\n",
        "\n",
        "#https://www.machinelearningplus.com/nlp/what-is-tokenization-in-natural-language-processing/\n",
        "\n",
        "#https://blog.floydhub.com/tokenization-nlp/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6qfM0b_0ztV"
      },
      "source": [
        "# Tokenization is a pre-processing step in NLP\n",
        "Tokenization is the process of demarcating and possibly classifying sections of a string of input characters  (wikipedia).\n",
        "\n",
        "\n",
        "*   Tokens can be words , characters , sub-words(n-grams).\n",
        "*   Creating vocabulary is the ultimate goal of tokenization.\n",
        "\n",
        "# Word Tokenization\n",
        "\n",
        "\n",
        "\n",
        "*   It is most commonly used\n",
        "*   It splits a piece text into individual words based on some delimiters\n",
        "\n",
        "\n",
        "*   Word2Vec and GloVe used for word tokenization\n",
        "*   Out of vocabulary (OOV) words are drawbacks\n",
        "*   OOVs are mapped with UNK (unknown) tokens\n",
        "*   Subword tokenizers are modern tokenizers\n",
        "*   BPE\n",
        "*   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtsXtEFS6FWz"
      },
      "source": [
        "# Bite Pair Encoding (BPE)  https://leimao.github.io/blog/Byte-Pair-Encoding/\n",
        "* BPE is a algorithm for sub word tokenizatiom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIPuTwlV3-lZ"
      },
      "source": [
        "# word "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZV_DfsM4BQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d4a124-9d2d-4a1e-d23f-eb96aff07d8b"
      },
      "source": [
        "#Line tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJq9QBgYK7Ri"
      },
      "source": [
        "sentences = '''\n",
        "The First sentence is about Python. The Second: about Django. You can learn Python,Django and Data Ananlysis here. \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRcMNugyLEKr"
      },
      "source": [
        "nltk_tokens = nltk.sent_tokenize(sentences)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCNgF39UTvfL",
        "outputId": "b39fddbf-5247-4b8a-840d-8d2cac8f8b6a"
      },
      "source": [
        "print(nltk_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\nThe First sentence is about Python.', 'The Second: about Django.', 'You can learn Python,Django and Data Ananlysis here.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_SoHrS1T5an"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK5UJmzXUTa1"
      },
      "source": [
        "# Non-English / Foreign language tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d40XXzEpUYzX",
        "outputId": "1373425f-b6ee-4a1d-990b-d16b1bf0fb98"
      },
      "source": [
        "german_tokenizer = nltk.data.load(\"tokenizers/punkt/german.pickle\")\n",
        "german_sentence_tokens= german_tokenizer.tokenize('Wie geht es dir heute')\n",
        "print(german_sentence_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Wie geht es dir heute']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovVfTPmgVfrV"
      },
      "source": [
        "# Word tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL4qIlFhViUI"
      },
      "source": [
        "import nltk\n",
        "word_dat ='''\n",
        "it's a beautiful day today , lets go cycling.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq0SjhEnV1qa",
        "outputId": "fec0db4b-d0ee-45a5-9a38-4c066f189662"
      },
      "source": [
        "word_tokens = nltk.word_tokenize(word_dat)\n",
        "print(word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['it', \"'s\", 'a', 'beautiful', 'day', 'today', ',', 'lets', 'go', 'cycling', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drFjPMdrWb_l"
      },
      "source": [
        "# Indic Language tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "733h94o4Wvsh",
        "outputId": "e2dd2485-ed85-4a23-a211-841e884d5caa"
      },
      "source": [
        "!pip install -q torch==1.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 169.1MB 56kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "547Z2OpYW0rJ",
        "outputId": "cb4bd672-b409-430d-b2d9-dd3a8132fa7e"
      },
      "source": [
        "!pip install -q inltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 81kB 2.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 26.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 41.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 34.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 34.6MB/s \n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqBRwug4WXBy"
      },
      "source": [
        "# Indic language tokenization\n",
        "import inltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBQeHTI3XXXa"
      },
      "source": [
        "odia_sentence ='ଆଜି ତୁମେ କେମିତି ଅଛ? ମୁଁ ତୁମକୁ ଦେଖି ଭଲ ଅନୁଭବ କରୁଛି |'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJEhHvRaXnTa"
      },
      "source": [
        "from inltk.inltk import tokenize\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "LBbFwcaSX9WC",
        "outputId": "5dda4f6a-3b2e-4993-e8a8-9db85f514653"
      },
      "source": [
        "from inltk.inltk import setup\n",
        "# setup('or')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f0c0c2665a50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# setup('or')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inltk'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO1wQREEXsoe"
      },
      "source": [
        "odia_tokens = tokenize(odia_sentence , 'or')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpI3Kw50YN4f",
        "outputId": "bc0bc8d0-845a-4b2f-cc4d-6930c18a8d35"
      },
      "source": [
        "print(odia_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁ଆଜି', '▁ତୁମେ', '▁କେମିତି', '▁ଅ', 'ଛ', '?', '▁ମୁଁ', '▁ତୁମକୁ', '▁ଦେଖି', '▁ଭଲ', '▁ଅନୁଭବ', '▁କରୁଛି', '▁|']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmqGt73vaieA"
      },
      "source": [
        "# Indic NLP library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUKFsQmia3Xq",
        "outputId": "b3a9ba88-8053-4c78-a536-da1b6cbf742f"
      },
      "source": [
        "!pip install -q folium==0.2.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▊                           | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20kB 7.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25h  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1ucE_-yalQN"
      },
      "source": [
        "!pip install -q indic-nlp-library\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}